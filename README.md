# OpenGraphAU Real-Time Video Emotion Classification and Action Unit Detection System
## Project Overview
This project is a real-time video processing system based on OpenGraphAU, capable of detecting and analyzing human facial expressions, recognizing emotions, and highlighting relevant action units (AUs). The system utilizes OpenCV for video frame capture, MediaPipe for face detection and keypoint localization, and ONNX Runtime for emotion recognition and action unit detection using pre-trained models.
## Technology Stack
- **OpenCV**: Used for video frame capture and processing.
- **MediaPipe**: Employed for real-time face detection and keypoint localization.
- **ONNX Runtime**: Utilized for loading and running pre-trained ONNX models.
- **Pygame**: Used for real-time display of processing results and user interface interaction.
- **Multiprocessing**: Implements multiprocessing using Python's `multiprocessing` module to enhance system real-time performance.
## Functional Modules
### 1. Video Frame Capture
- Captures video frames in real-time using the OpenCV library.
- Supports setting of frame rate and resolution.
### 2. Face Detection and Keypoint Localization
- Utilizes MediaPipe's `FaceDetection` module for face detection.
- Extracts key facial points, including the positions of eyes, nose, and mouth.
### 3. Emotion Recognition and Action Unit Detection
- Uses pre-trained ONNX models for emotion recognition.
- Detects relevant action units (AUs) and maps them to facial regions.
### 4. Real-Time Display and Interaction
- Uses the Pygame library to display processed video frames in real-time.
- Draws action units and emotion recognition results.
- Supports user interaction, such as displaying detailed information about action units on mouse hover.
### 5. Multiprocessing
- Adopts a multiprocessing architecture, separating video frame capture, face detection, emotion recognition, and result display into different processes.
- Synchronizes data using inter-process communication (e.g., queues) to ensure real-time performance.
## Usage Instructions
1. Ensure all dependencies are installed. Run `pip install -r requirements.txt`
2. prepare graph_au_model.onnx(![]), the onnx file can be generated by OpengraphAU. 
3. Run `python test_camera.py` to start the system.
4. The system will automatically open the camera and display real-time processing results. You can upload your own icon.png in figs and change it visible in test_camera.py

## Acknowledgments
Thanks to all the developers who have contributed code and suggestions to this project.
